{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "L3Mf8rFUiL35"
   },
   "outputs": [],
   "source": [
    "# !pip install bcolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "KaDfOEV76MAr"
   },
   "outputs": [],
   "source": [
    "# References\n",
    "# - https://classroom.udacity.com/courses/ud188\n",
    "# - https://jovian.ai/aakashns/05b-cifar10-resnet\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import tarfile\n",
    "import zipfile\n",
    "import shutil\n",
    "import json\n",
    "import pickle\n",
    "# import bcolz\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.models as models\n",
    "from torchtext.vocab import Vocab\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "matplotlib.rcParams[\"figure.facecolor\"] = \"#ffffff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "qgmYRfBw3w8U"
   },
   "outputs": [],
   "source": [
    "# Extract from zip\n",
    "# Train images\n",
    "# with zipfile.ZipFile('/content/drive/MyDrive/Colab Notebooks/FYP/VQA_Reqog/Dataset/dataset_zips/train2014.zip', 'r') as zip_ref:\n",
    "#    zip_ref.extractall('./')\n",
    "\n",
    "# Val images\n",
    "# with zipfile.ZipFile('/content/drive/MyDrive/Colab Notebooks/FYP/VQA_Reqog/Dataset/dataset_zips/val2014.zip', 'r') as zip_ref:\n",
    "#    zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "R1JXnNb07dFT"
   },
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "# IF CHANGING ANY PARAMETERS OR THE MODEL, CHANGE THE MODEL NAME FROM HERE AND NOTE IT IN THE EXCEL SHEET\n",
    "# PROJECT_NAME = \"reqog\"\n",
    "\n",
    "# 23 is the max length of the all the questions in the dataset\n",
    "# SEQ_LENGTH = 25\n",
    "# BATCH_SIZE = 2\n",
    "# MODEL_NAME = \"test-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "jU83RLoG6zAB"
   },
   "outputs": [],
   "source": [
    "# Custom VQA dataset class\n",
    "class VQADataset(Dataset):\n",
    "  size = None\n",
    "  answer_type = None\n",
    "  question_types = None\n",
    "  num_of_output_classes = None\n",
    "  train_json_annotations = None\n",
    "  val_json_annotations = None\n",
    "  ans_vocab_to_int = None\n",
    "  vocab_to_int = None\n",
    "  train_img_list = []\n",
    "  val_img_list = []\n",
    "\n",
    "  def __init__(self, dataset_type, size=None, answer_type=None, question_types=None, num_of_output_classes=None, transforms=None):\n",
    "    if (answer_type is not None) and (num_of_output_classes is not None or size is not None):\n",
    "      print(\"ERROR: Only one of the 3 filters can be used\")\n",
    "      return\n",
    "\n",
    "    if (num_of_output_classes is not None) and (answer_type is not None or size is not None):\n",
    "      print(\"ERROR: Only one of the 3 filters can be used\")\n",
    "      return\n",
    "\n",
    "    if (size is not None) and (answer_type is not None or num_of_output_classes is not None):\n",
    "      print(\"ERROR: Only one of the 3 filters can be used\")\n",
    "      return\n",
    "\n",
    "    self.dataset_type = dataset_type\n",
    "    VQADataset.size = size\n",
    "    VQADataset.answer_type = answer_type\n",
    "    VQADataset.question_types = question_types\n",
    "    VQADataset.num_of_output_classes = num_of_output_classes\n",
    "    self.transforms = transforms\n",
    "\n",
    "    # Setting the file paths and the annotations file data\n",
    "    self.root_dir = \"./\"\n",
    "\n",
    "    # Train\n",
    "    train_images_dir_path = self.root_dir + \"train2014\"\n",
    "    train_annotations_file_path = self.root_dir + \"drive/MyDrive/Colab Notebooks/FYP/VQA_Reqog/Dataset/train_data.json\"\n",
    "    VQADataset.train_img_list = os.listdir(train_images_dir_path)\n",
    "    with open(train_annotations_file_path, \"r\") as openfile: \n",
    "      VQADataset.train_json_annotations = json.load(openfile)\n",
    "\n",
    "    # Val\n",
    "    val_images_dir_path = self.root_dir + \"val2014\"\n",
    "    val_annotations_file_path = self.root_dir + \"drive/MyDrive/Colab Notebooks/FYP/VQA_Reqog/Dataset/val_data.json\"\n",
    "    VQADataset.val_img_list = os.listdir(val_images_dir_path)\n",
    "    with open(val_annotations_file_path, \"r\") as openfile: \n",
    "      VQADataset.val_json_annotations = json.load(openfile)\n",
    "\n",
    "    # Main data for specific dataset types\n",
    "    self.data = []\n",
    "\n",
    "    # Filtering using the question_type (eg: \"how many\")\n",
    "    if VQADataset.question_types is not None:\n",
    "      train_data = []\n",
    "      val_data = []\n",
    "      for question in VQADataset.train_json_annotations:\n",
    "        if question[\"question_type\"] in VQADataset.question_types:\n",
    "          train_data.append(question)\n",
    "\n",
    "      for question in VQADataset.val_json_annotations:\n",
    "        if question[\"question_type\"] in VQADataset.question_types:\n",
    "          val_data.append(question)\n",
    "      \n",
    "      VQADataset.train_json_annotations = train_data\n",
    "      VQADataset.val_json_annotations = val_data\n",
    "\n",
    "    # Filtering using the answer_type (\"yes/no\", \"number\", and \"other\")\n",
    "    if VQADataset.answer_type is not None:\n",
    "      train_data = []\n",
    "      val_data = []\n",
    "      for question in VQADataset.train_json_annotations:\n",
    "        if question[\"answer_type\"] == VQADataset.answer_type and question[\"answer_type\"] == \"yes/no\":\n",
    "          if question[\"answer\"] == \"yes\" or question[\"answer\"] == \"no\":\n",
    "            train_data.append(question)  \n",
    "        elif question[\"answer_type\"] == VQADataset.answer_type:\n",
    "          train_data.append(question)\n",
    "\n",
    "      for question in VQADataset.val_json_annotations:\n",
    "        if question[\"answer_type\"] == VQADataset.answer_type and question[\"answer_type\"] == \"yes/no\":\n",
    "          if question[\"answer\"] == \"yes\" or question[\"answer\"] == \"no\":\n",
    "            val_data.append(question)  \n",
    "        elif question[\"answer_type\"] == VQADataset.answer_type:\n",
    "          val_data.append(question)\n",
    "      \n",
    "      VQADataset.train_json_annotations = train_data\n",
    "      VQADataset.val_json_annotations = val_data\n",
    "\n",
    "    # Filtering using the num_of_output_classes\n",
    "    if VQADataset.num_of_output_classes is not None:\n",
    "      train_data = []\n",
    "      val_data = []\n",
    "      train_answers = []\n",
    "      for question in VQADataset.train_json_annotations:\n",
    "        train_answers.append(question[\"answer\"])\n",
    "\n",
    "      most_common_n_answers = [word for word, word_count in Counter(train_answers).most_common(VQADataset.num_of_output_classes)]    \n",
    "      \n",
    "      for question in VQADataset.train_json_annotations:\n",
    "        if question[\"answer\"] in most_common_n_answers:\n",
    "          train_data.append(question)\n",
    "\n",
    "      for question in VQADataset.val_json_annotations:\n",
    "        if question[\"answer\"] in most_common_n_answers:\n",
    "          val_data.append(question)  \n",
    "\n",
    "      VQADataset.train_json_annotations = train_data\n",
    "      VQADataset.val_json_annotations = val_data\n",
    "\n",
    "    # Filtering using the size\n",
    "    if VQADataset.size is not None:\n",
    "      VQADataset.train_json_annotations = VQADataset.train_json_annotations[0:VQADataset.size]\n",
    "      VQADataset.val_json_annotations = VQADataset.val_json_annotations[0:VQADataset.size]\n",
    "\n",
    "    # Create the vocab for questions and answers\n",
    "    # Adds the individual questions in the train set to the all_questions_list list\n",
    "    all_questions_list = []\n",
    "    all_answers_list = []\n",
    "    for item in VQADataset.train_json_annotations:\n",
    "      all_questions_list.append(\" \".join(item[\"question\"].split()))\n",
    "      all_answers_list.append(item[\"answer\"])\n",
    "\n",
    "    for item in VQADataset.val_json_annotations:\n",
    "      all_questions_list.append(\" \".join(item[\"question\"].split()))\n",
    "      all_answers_list.append(item[\"answer\"])\n",
    "\n",
    "    # Joining all questions using a new line \n",
    "    all_questions_string_with_newline_separator = \"\\n\".join(all_questions_list)\n",
    "\n",
    "    # Converting all the text to lowecase\n",
    "    all_questions_string_with_newline_separator = all_questions_string_with_newline_separator.lower()\n",
    "\n",
    "    # Getting rid of punctuation\n",
    "    all_text = \"\".join([char for char in all_questions_string_with_newline_separator if char not in punctuation])\n",
    "\n",
    "    # Split by new lines\n",
    "    questions_split = all_text.split(\"\\n\")\n",
    "\n",
    "    # Joining each and every element in the list by a \" \" to make a \n",
    "    # single string that contains all the words from the question\n",
    "    all_text = \" \".join(questions_split)\n",
    "\n",
    "    # Create a list of words\n",
    "    words = all_text.split()\n",
    "\n",
    "    # Encoding the words of questions and answers\n",
    "    # Build a dictionary that maps words to integers\n",
    "    counts = Counter(words)\n",
    "    ans_counts = Counter(all_answers_list)\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "    ans_vocab = sorted(ans_counts, key=ans_counts.get, reverse=True)\n",
    "    VQADataset.vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "    VQADataset.ans_vocab_to_int = {word: ii for ii, word in enumerate(ans_vocab)}\n",
    "    VQADataset.vocab_to_int = self.update_vocab_with_unknown_token()\n",
    "\n",
    "    print(\"Q vocab:\", VQADataset.vocab_to_int)\n",
    "    print(\"A vocab:\", VQADataset.ans_vocab_to_int)\n",
    "\n",
    "    # Use the dict to tokenize each question in questions_split and answer\n",
    "    # Store the tokenized questions in questions_ints and answers in answer_ints\n",
    "    questions_ints = []\n",
    "    answer_ints = []\n",
    "    for question in questions_split:\n",
    "      questions_ints.append([VQADataset.vocab_to_int[word] for word in question.split()])\n",
    "\n",
    "    for answer in all_answers_list:\n",
    "      answer_ints.append(VQADataset.ans_vocab_to_int[answer])\n",
    "\n",
    "    # Outlier questions stats\n",
    "    question_lens = Counter([len(x) for x in questions_ints])\n",
    "    print(\"Zero length questions: {}\".format(question_lens[0]))\n",
    "    print(\"Maximum question length: {}\".format(max(question_lens)))\n",
    "\n",
    "    question_features = self.pad_features(questions_ints, seq_length=SEQ_LENGTH)\n",
    "\n",
    "    # Test statements\n",
    "    assert len(question_features)==len(questions_ints), \"Your features should have as many rows as questions.\"\n",
    "    assert len(question_features[0])==SEQ_LENGTH, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "    print(\"question features len:\", len(question_features))\n",
    "    print(\"answer ints len:\", len(answer_ints))\n",
    "    print(\"trainset len:\", len(VQADataset.train_json_annotations))\n",
    "    print(\"valset len:\", len(VQADataset.val_json_annotations))\n",
    "\n",
    "    # Take the question_features and answer_ints and add to the val and train _json_annotations list\n",
    "    train_len = len(VQADataset.train_json_annotations)\n",
    "    train_question_features, val_question_features = question_features[:train_len], question_features[train_len:]\n",
    "    train_answer_ints, val_answer_ints = answer_ints[:train_len], answer_ints[train_len:]\n",
    "\n",
    "    print(\"train_question_features len:\", len(train_question_features))\n",
    "    print(\"val_question_features len:\", len(val_question_features))\n",
    "    print(\"train_answer_ints len:\", len(train_answer_ints))\n",
    "    print(\"val_answer_ints len:\", len(val_answer_ints))\n",
    "\n",
    "    for i, item in enumerate(VQADataset.train_json_annotations):\n",
    "      item[\"encoded_question\"] = train_question_features[i]\n",
    "      item[\"encoded_answer\"] = train_answer_ints[i]\n",
    "\n",
    "    for i, item in enumerate(VQADataset.val_json_annotations):\n",
    "      item[\"encoded_question\"] = val_question_features[i]\n",
    "      item[\"encoded_answer\"] = val_answer_ints[i]\n",
    "\n",
    "    # Assigning the data variable based on the dataset_type\n",
    "    valset, testset = self.split_list(VQADataset.val_json_annotations)\n",
    "    if self.dataset_type == \"Train\":\n",
    "      self.data = VQADataset.train_json_annotations\n",
    "      self.img_dir = train_images_dir_path\n",
    "    elif self.dataset_type == \"Val\":\n",
    "      self.data = valset\n",
    "      self.img_dir = val_images_dir_path\n",
    "    elif self.dataset_type == \"Test\":\n",
    "      self.data = testset\n",
    "      self.img_dir = val_images_dir_path\n",
    "\n",
    "  def split_list(self, a_list):\n",
    "    half = len(a_list)//2\n",
    "    return a_list[:half], a_list[half:]   \n",
    "\n",
    "  def update_vocab_with_unknown_token(self):\n",
    "    unknown_token = {'<unk>': 0}\n",
    "    unknown_token.update(VQADataset.vocab_to_int)\n",
    "    num = 1\n",
    "    for key in unknown_token:\n",
    "      unknown_token[key] = num\n",
    "      num += 1\n",
    "\n",
    "    return unknown_token\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # Getting the featurized question\n",
    "    individual_datapoint = self.data[index] \n",
    "\n",
    "    # Getting the featurized question\n",
    "    featurized_question = individual_datapoint[\"encoded_question\"]\n",
    "    \n",
    "    # Getting the image\n",
    "    image_path = self.img_dir + \"/\" + individual_datapoint[\"img_filename\"]\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Getting the answers\n",
    "    answer = individual_datapoint[\"encoded_answer\"]\n",
    "\n",
    "    # Transforming the image if transforms are specifed\n",
    "    if self.transforms is not None:\n",
    "      image = self.transforms(image)\n",
    "\n",
    "    return featurized_question, image, answer\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def pad_features(self, questions_ints, seq_length):\n",
    "    # Return features of questions_ints, where each question is padded with 0's \n",
    "    # or truncated to the input seq_length.\n",
    "    \n",
    "    # Getting the correct rows x cols shape\n",
    "    features = np.zeros((len(questions_ints), seq_length), dtype=int)\n",
    "\n",
    "    # For each question, I grab that question and add zeros to the start \n",
    "    # or truncate to the seq_length\n",
    "    for i, row in enumerate(questions_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "JkX2WI2ffyT9"
   },
   "outputs": [],
   "source": [
    "# If the image doesn't look like the original it is probably \n",
    "# because of the transformations\n",
    "# def show_example(question, image, answer, q_vocab, a_vocab):\n",
    "#     plt.imshow(image.permute(1, 2, 0))\n",
    "\n",
    "#     print(\"Encoded question:\", question)\n",
    "#     print(q_vocab)\n",
    "#     decoded_q = \"\"\n",
    "#     for word in question:\n",
    "#       for w, num in q_vocab.items():\n",
    "#         if word == num:\n",
    "#           decoded_q += w + \" \"\n",
    "#     print(\"Decoded question:\", decoded_q)\n",
    "\n",
    "#     print(\"Encoded ground truth answer:\", answer)\n",
    "#     print(a_vocab)\n",
    "#     decoded_ans = \"\"\n",
    "#     for ans, num in a_vocab.items():\n",
    "#       if num == answer:\n",
    "#         decoded_ans = ans\n",
    "#     print(\"Decoded ground truth answer:\", decoded_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "UyjZ2Vm_5_gb"
   },
   "outputs": [],
   "source": [
    "# d_means = [0.485, 0.456, 0.406]\n",
    "# d_stds = [0.229, 0.224, 0.225]\n",
    "# img_transforms = tt.Compose([tt.Resize((224, 224)),\n",
    "#                                tt.ToTensor(),\n",
    "#                                tt.Normalize(mean=d_means, std=d_stds)\n",
    "#                                ])\n",
    "\n",
    "# question_types = [\"how many\", \"how many people are\", \"how many people are in\"]\n",
    "\n",
    "# # Filter data using parameters\n",
    "# train_set = VQADataset(dataset_type=\"Train\", size=10, transforms=img_transforms)\n",
    "# val_set = VQADataset(dataset_type=\"Val\", size=10, transforms=img_transforms)\n",
    "# test_set = VQADataset(dataset_type=\"Test\", size=10, transforms=img_transforms)\n",
    "\n",
    "# print(\"Length of train set:\", len(train_set))\n",
    "# print(\"Length of val set:\", len(val_set))\n",
    "# print(\"Length of test set:\", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "F1ye0EjbRZ0s"
   },
   "outputs": [],
   "source": [
    "# show_example(*test_set[0], VQADataset.vocab_to_int, VQADataset.ans_vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "dkw4cdCeidu7"
   },
   "outputs": [],
   "source": [
    "# train_dataloader = DataLoader(train_set, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "# val_dataloader = DataLoader(val_set, BATCH_SIZE*2, num_workers=2, pin_memory=True)\n",
    "# test_dataloader = DataLoader(test_set, BATCH_SIZE*2, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "zgEYtjv4l1Aw"
   },
   "outputs": [],
   "source": [
    "# Testing purpose\n",
    "# for questions, images, answers in train_dataloader:\n",
    "#   print(questions.shape)\n",
    "#   print(images.shape)\n",
    "#   print(answers.shape)\n",
    "#   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "MQNvv62Ei4mj"
   },
   "outputs": [],
   "source": [
    "# def denormalize(images, means, stds):\n",
    "#     means = torch.tensor(means).reshape(1, 3, 1, 1)\n",
    "#     stds = torch.tensor(stds).reshape(1, 3, 1, 1)\n",
    "#     return images * stds + means\n",
    "\n",
    "# def show_batch(dataloader):\n",
    "#     for questions, images, answers in dataloader:\n",
    "#         fig, ax = plt.subplots(figsize=(12, 12))\n",
    "#         ax.set_xticks([]); ax.set_yticks([])\n",
    "#         denorm_images = denormalize(images, d_means, d_stds)\n",
    "\n",
    "#         # make_grid(denorm_images[:64], nrow=8) max of 64 images in the \n",
    "#         # grid with 8 images in each row \n",
    "#         ax.imshow(make_grid(denorm_images[:64], nrow=8).permute(1, 2, 0).clamp(0,1))\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "dP5Nhf4sjWYA"
   },
   "outputs": [],
   "source": [
    "# show_batch(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "RzpFsDFo6Wwk"
   },
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "class VQABase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        questions, images, answers = batch \n",
    "        \n",
    "        # Generate predictions\n",
    "        out = self(images, questions)                             \n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(out, answers)           \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        questions, images, answers = batch\n",
    "        \n",
    "        # Generate predictions\n",
    "        out = self(images, questions)                 \n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(out, answers)   \n",
    "        \n",
    "        # Calculate accuracy\n",
    "        acc = accuracy(out, answers)           \n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        \n",
    "        # Combine losses\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   \n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        \n",
    "        # Combine accuracies\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      \n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch: {}, last_lr: {:.5f}, training_loss: {:.4f}, validation_loss: {:.4f}, validation_accuracy: {:.4f}\".format(\n",
    "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "YWzOl5XxE5X5"
   },
   "outputs": [],
   "source": [
    "# Creates the embedding layer with GloVe embeddings\n",
    "def create_emb_layer(weights_matrix, non_trainable=False):\n",
    "  num_embeddings, embedding_dim = weights_matrix.shape\n",
    "  emb_layer = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n",
    "  emb_layer.load_state_dict({'weight': torch.Tensor(weights_matrix)})\n",
    "  if non_trainable:\n",
    "      emb_layer.weight.requires_grad = False\n",
    "\n",
    "  return emb_layer, num_embeddings, embedding_dim\n",
    "\n",
    "class VQAModel(VQABase):\n",
    "  def __init__(self, num_classes, weights_matrix):\n",
    "    super(VQAModel, self).__init__()\n",
    "\n",
    "    # Image\n",
    "    # Get the pretrained resnet model\n",
    "    resnet152 = models.resnet152(pretrained=True)\n",
    "\n",
    "    # Remove the final fully connected layers to get only the convolutional \n",
    "    # feature extraction part of the CNN(ResNet)\n",
    "    modules = list(resnet152.children())[:-1]\n",
    "    resnet152 = nn.Sequential(*modules)\n",
    "\n",
    "    # Fix the parameters of the feature extractor so that when training the \n",
    "    # gradients do not get calculated and therefore no saving memory\n",
    "    for p in resnet152.parameters():\n",
    "      p.requires_grad = False\n",
    "\n",
    "    resnet152.add_module(\"flatten\", nn.Flatten())\n",
    "    resnet152.add_module(\"relu\", nn.ReLU())\n",
    "\n",
    "    self.image_feature_extractor = resnet152\n",
    "\n",
    "    # Question\n",
    "    self.embedding, num_embeddings, embedding_dim = create_emb_layer(weights_matrix, True)\n",
    "    self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=512, num_layers=2, batch_first=True)\n",
    "    self.lstm_fc = nn.Linear(512, 2048)\n",
    "    self.dropout = nn.Dropout(0.2)\n",
    "    print(\"Num classes:\", num_classes)\n",
    "    self.classifier = nn.Linear(2048, num_classes)\n",
    "\n",
    "  def forward(self, enc_img, enc_q):\n",
    "    img_feats = self.image_feature_extractor(enc_img)\n",
    "    \n",
    "    embeddings = self.embedding(enc_q)\n",
    "    embeddings = self.dropout(embeddings)\n",
    "    q_out, (ht, ct) = self.lstm(embeddings)\n",
    "\n",
    "    final_pred = torch.mul(img_feats, self.lstm_fc(ht[-1]))\n",
    "    final_pred = self.classifier(final_pred)\n",
    "\n",
    "    return final_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "ivdY7oJRgLkZ"
   },
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "\n",
    "    # Pick GPU if available, else CPU\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "\n",
    "    # Move tensor(s) to chosen device\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "  \n",
    "    # Wrap a dataloader to move data to a device\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    # Yield a batch of data after moving it to device\n",
    "    def __iter__(self):\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    # Number of batches\n",
    "    def __len__(self):\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "VOFBvfFwurcp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "IDBGh5M4u0M4"
   },
   "outputs": [],
   "source": [
    "# train_dataloader = DeviceDataLoader(train_dataloader, device)\n",
    "# val_dataloader = DeviceDataLoader(val_dataloader, device)\n",
    "# test_dataloader = DeviceDataLoader(test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "baQcCN72CYpK"
   },
   "outputs": [],
   "source": [
    "# Creating the GloVe embeddings\n",
    "# glove_path = '/content/drive/MyDrive/Colab Notebooks/FYP/VQA_Reqog/GloVe'\n",
    "\n",
    "# vectors = bcolz.open(f'{glove_path}/6B.300.dat')[:]\n",
    "# words = pickle.load(open(f'{glove_path}/6B.300_words.pkl', 'rb'))\n",
    "# word2idx = pickle.load(open(f'{glove_path}/6B.300_idx.pkl', 'rb'))\n",
    "\n",
    "# glove = {w: vectors[word2idx[w]] for w in words}\n",
    "\n",
    "# matrix_len = len(VQADataset.vocab_to_int) + 1\n",
    "# weights_matrix = np.zeros((matrix_len, 300))\n",
    "# words_found = 0\n",
    "\n",
    "# for i, word in enumerate(VQADataset.vocab_to_int):\n",
    "#     try: \n",
    "#         weights_matrix[i + 1] = glove[word]\n",
    "#         words_found += 1\n",
    "#     except KeyError:\n",
    "#         weights_matrix[i + 1] = np.random.normal(scale=0.6, size=(300, ))\n",
    "\n",
    "# weights_matrix[1] = pickle.load(open(f'{glove_path}/unk_embeddings_list.pkl', 'rb'))\n",
    "# print(\"Total length of vocab:\", matrix_len)\n",
    "# print(\"Words found:\", words_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "ogg4igFv7L-E"
   },
   "outputs": [],
   "source": [
    "# model = to_device(VQAModel(len(VQADataset.ans_vocab_to_int), weights_matrix), device)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "VblYdkZc71aO"
   },
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "# def evaluate(model, val_loader):\n",
    "#     model.eval()\n",
    "#     outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "#     return model.validation_epoch_end(outputs)\n",
    "\n",
    "# def get_lr(optimizer):\n",
    "#     for param_group in optimizer.param_groups:\n",
    "#         return param_group['lr']\n",
    "\n",
    "# def fit_one_cycle(epochs, tot_epochs, model, history, train_loader, val_loader, \n",
    "#                   learning_rate_optimizer, one_cycle_scheduler, grad_clip=None):\n",
    "#     torch.cuda.empty_cache()\n",
    "    \n",
    "#     for epoch in range(epochs):\n",
    "\n",
    "#         # Training Phase \n",
    "#         model.train()\n",
    "#         train_losses = []\n",
    "#         lrs = []\n",
    "#         for batch in train_loader:\n",
    "#             loss = model.training_step(batch)\n",
    "#             train_losses.append(loss)\n",
    "#             loss.backward()\n",
    "            \n",
    "#             # Gradient clipping\n",
    "#             if grad_clip: \n",
    "#                 nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "#             learning_rate_optimizer.step()\n",
    "#             learning_rate_optimizer.zero_grad()\n",
    "            \n",
    "#             # Record & update learning rate\n",
    "#             lrs.append(get_lr(learning_rate_optimizer))\n",
    "#             one_cycle_scheduler.step()\n",
    "        \n",
    "#         # Validation phase\n",
    "#         result = evaluate(model, val_loader)\n",
    "#         result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "#         result['lrs'] = lrs\n",
    "#         model.epoch_end(epoch, result)\n",
    "#         history.append(result)\n",
    "\n",
    "#         # Saving the model\n",
    "#         remaining_epochs = (epochs - (epoch + 1))\n",
    "#         epoch_file_num = (tot_epochs - remaining_epochs)\n",
    "\n",
    "#         # Save new model checkpoint\n",
    "#         torch.save({\n",
    "#             'model_state_dict': model.state_dict(),\n",
    "#             'optimizer_state_dict': learning_rate_optimizer.state_dict(),\n",
    "#             'sched_state_dict': one_cycle_scheduler.state_dict(),\n",
    "#             'history': history,\n",
    "#             'remaining_epochs': remaining_epochs\n",
    "#             }, './drive/MyDrive/Colab Notebooks/FYP/Saved models/epoch_no_' + str(epoch_file_num) + '-' + MODEL_NAME + '.pth')\n",
    "#     return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "rI7NzsFm8oQl"
   },
   "outputs": [],
   "source": [
    "# history = [evaluate(model, val_dataloader)]\n",
    "# history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "zSxV5vYBVb1U"
   },
   "outputs": [],
   "source": [
    "# evaluate(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "hEJSiUrzv6q4"
   },
   "outputs": [],
   "source": [
    "# print(\"Outputs before saving:\", VQADataset.ans_vocab_to_int)\n",
    "# print(\"Vocab before saving:\", VQADataset.vocab_to_int)\n",
    "\n",
    "# EPOCHS = 8\n",
    "# MAX_LR = 0.01\n",
    "# GRAD_CLIP = 0.1\n",
    "# WEIGHT_DECAY = 1e-4\n",
    "# opt_func = torch.optim.Adam\n",
    "\n",
    "# # Set up cutom optimizer with weight decay\n",
    "# learning_rate_optimizer = opt_func(model.parameters(), MAX_LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# # Set up one-cycle learning rate scheduler\n",
    "# one_cycle_scheduler = torch.optim.lr_scheduler.OneCycleLR(learning_rate_optimizer, \n",
    "#                       MAX_LR, epochs=EPOCHS, steps_per_epoch=len(train_dataloader))\n",
    "\n",
    "# # Save model init checkpoint\n",
    "# torch.save({\n",
    "#             'model_structure': model,\n",
    "#             'optimizer_structure': learning_rate_optimizer,\n",
    "#             'sched_structure': one_cycle_scheduler,\n",
    "#             'ans_vocab_to_int': VQADataset.ans_vocab_to_int,\n",
    "#             'vocab_to_int': VQADataset.vocab_to_int,\n",
    "#             'epochs': EPOCHS,\n",
    "#             'max_lr': MAX_LR,\n",
    "#             'weight_decay': WEIGHT_DECAY,\n",
    "#             'grad_clip': GRAD_CLIP,\n",
    "#             'opt_func': opt_func,\n",
    "#             'history': history\n",
    "#             }, './drive/MyDrive/Colab Notebooks/FYP/Saved models/init-' + MODEL_NAME + '.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "BXOhrVr9HG2b"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# history = fit_one_cycle(epochs=EPOCHS, tot_epochs=EPOCHS, model=model, history=history, \n",
    "#                           train_loader=train_dataloader, val_loader=val_dataloader, \n",
    "#                           learning_rate_optimizer=learning_rate_optimizer, \n",
    "#                          one_cycle_scheduler=one_cycle_scheduler, grad_clip=GRAD_CLIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "1_p1F7NY2h3h"
   },
   "outputs": [],
   "source": [
    "# Load from init checkpoint\n",
    "# init_checkpoint = torch.load('./drive/MyDrive/Colab Notebooks/FYP/Saved models/init-' + MODEL_NAME + '.pth')\n",
    "# init_model_structure = init_checkpoint['model_structure']\n",
    "# init_optimizer_structure = init_checkpoint['optimizer_structure']\n",
    "# init_sched_structure = init_checkpoint['sched_structure']\n",
    "# init_ans_vocab_to_int = init_checkpoint['ans_vocab_to_int']\n",
    "# init_vocab_to_int = init_checkpoint['vocab_to_int']\n",
    "# init_epochs = init_checkpoint['epochs']\n",
    "# init_max_lr = init_checkpoint['max_lr']\n",
    "# init_weight_decay = init_checkpoint['weight_decay']\n",
    "# init_grad_clip = init_checkpoint['grad_clip']\n",
    "# init_opt_func = init_checkpoint['opt_func']\n",
    "# init_history = init_checkpoint['history']\n",
    "\n",
    "# # Load from last epoch checkpoint\n",
    "# # Enter epoch number\n",
    "# # last_checkpoint = torch.load('./drive/MyDrive/Colab Notebooks/FYP/Saved models/epoch_no_8-' + MODEL_NAME + '.pth')\n",
    "# last_model_state_dict = last_checkpoint['model_state_dict']\n",
    "# last_optimizer_state_dict = last_checkpoint['optimizer_state_dict']\n",
    "# last_sched_state_dict = last_checkpoint['sched_state_dict']\n",
    "# last_history = last_checkpoint['history']\n",
    "# last_remaining_epochs = last_checkpoint['remaining_epochs']\n",
    "\n",
    "# init_model_structure.load_state_dict(last_model_state_dict)\n",
    "# init_optimizer_structure.load_state_dict(last_optimizer_state_dict)\n",
    "# init_sched_structure.load_state_dict(last_sched_state_dict)\n",
    "\n",
    "# if last_remaining_epochs == 0:\n",
    "#   print(\"Already trained for the full amount of epochs\")\n",
    "# else:\n",
    "#   print(last_remaining_epochs, \"epoch(s) trainable, uncomment the below cell and run it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "aoQR9H_O6kJ6"
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# history = fit_one_cycle(epochs=last_remaining_epochs, tot_epochs=init_epochs, model=init_model_structure, history=last_history, \n",
    "#                           train_loader=train_dataloader, val_loader=val_dataloader, \n",
    "#                           learning_rate_optimizer=init_optimizer_structure, \n",
    "#                          one_cycle_scheduler=init_sched_structure, grad_clip=init_grad_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "ZMcNSOt8SB8J"
   },
   "outputs": [],
   "source": [
    "# def plot_accuracies(history):\n",
    "#     accuracies = [x['val_acc'] for x in history]\n",
    "#     plt.plot(accuracies, '-x')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.ylabel('accuracy')\n",
    "#     plt.title('Accuracy vs. No. of epochs')\n",
    "\n",
    "# def plot_losses(history):\n",
    "#     train_losses = [x.get('train_loss') for x in history]\n",
    "#     val_losses = [x['val_loss'] for x in history]\n",
    "#     plt.plot(train_losses, '-bx')\n",
    "#     plt.plot(val_losses, '-rx')\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.ylabel('loss')\n",
    "#     plt.legend(['Training', 'Validation'])\n",
    "#     plt.title('Loss vs. No. of epochs')\n",
    "\n",
    "# def plot_lrs(history):\n",
    "#     lrs = np.concatenate([x.get('lrs', []) for x in history])\n",
    "#     plt.plot(lrs)\n",
    "#     plt.xlabel('Batch no.')\n",
    "#     plt.ylabel('Learning rate')\n",
    "#     plt.title('Learning Rate vs. Batch no.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "3npUUOIjSD1U"
   },
   "outputs": [],
   "source": [
    "# plot_accuracies(last_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "xRnzRIB_SH4i"
   },
   "outputs": [],
   "source": [
    "# plot_losses(last_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "pp9LOvCaSX4B"
   },
   "outputs": [],
   "source": [
    "# plot_lrs(last_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "6PFhZCLfTA6F"
   },
   "outputs": [],
   "source": [
    "# question, img, answer = test_set[0]\n",
    "# show_example(question, img, answer, init_vocab_to_int, init_ans_vocab_to_int)\n",
    "# print()\n",
    "# print('Predicted:', predict_image(question, img, init_model_structure))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_QzcH1i2gHU"
   },
   "source": [
    "## ------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "kKFHAaWB7CgY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load from count model init checkpoint\n",
    "init_count_checkpoint = torch.load('E:\\Projects\\Reqog (FYP) - backend\\count_1.0-model\\init-count_1.0-model.pth', map_location=torch.device('cpu'))\n",
    "init_count_model_structure = init_count_checkpoint['model_structure']\n",
    "init_count_ans_vocab_to_int = init_count_checkpoint['ans_vocab_to_int']\n",
    "init_count_vocab_to_int = init_count_checkpoint['vocab_to_int']\n",
    "\n",
    "# Load from last epoch count model checkpoint\n",
    "# Enter epoch number\n",
    "last_count_checkpoint = torch.load('E:\\Projects\\Reqog (FYP) - backend\\count_1.0-model\\epoch_no_8-count_1.0-model.pth', map_location=torch.device('cpu'))\n",
    "last_count_model_state_dict = last_count_checkpoint['model_state_dict']\n",
    "last_count_history = last_count_checkpoint['history']\n",
    "\n",
    "init_count_model_structure.load_state_dict(last_count_model_state_dict)\n",
    "\n",
    "# Load from general model init checkpoint\n",
    "init_general_checkpoint = torch.load('E:\\Projects\\Reqog (FYP) - backend\\count_1.0-model\\init-count_1.0-model.pth', map_location=torch.device('cpu'))\n",
    "init_general_model_structure = init_general_checkpoint['model_structure']\n",
    "init_general_ans_vocab_to_int = init_general_checkpoint['ans_vocab_to_int']\n",
    "init_general_vocab_to_int = init_general_checkpoint['vocab_to_int']\n",
    "\n",
    "# Load from last epoch general model checkpoint\n",
    "# Enter epoch number\n",
    "last_general_checkpoint = torch.load('E:\\Projects\\Reqog (FYP) - backend\\count_1.0-model\\epoch_no_8-count_1.0-model.pth', map_location=torch.device('cpu'))\n",
    "last_general_model_state_dict = last_general_checkpoint['model_state_dict']\n",
    "last_general_history = last_general_checkpoint['history']\n",
    "\n",
    "init_general_model_structure.load_state_dict(last_general_model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [22/Apr/2021 15:19:50] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [22/Apr/2021 15:20:26] \"\u001b[37mPOST /get-prediction HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded question: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   3\n",
      "   2   4   7   6   5 241   8]\n",
      "Decoded question: how many are there in the grass people \n",
      "Predicted Answer: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Apr/2021 15:44:59] \"\u001b[37mPOST /get-prediction HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded question: [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   3\n",
      "   2   4   7   6   5 241   8]\n",
      "Decoded question: how many are there in the grass people \n",
      "Predicted Answer: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [22/Apr/2021 16:05:54] \"\u001b[37mPOST /get-prediction HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded question: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0 2435   25   97]\n",
      "Decoded question: what is up \n",
      "Predicted Answer: 0\n"
     ]
    }
   ],
   "source": [
    "# from flask_ngrok import run_with_ngrok\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "# Running the flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Start ngrok when the app is run\n",
    "# run_with_ngrok(app)\n",
    "\n",
    "d_means = [0.485, 0.456, 0.406]\n",
    "d_stds = [0.229, 0.224, 0.225]\n",
    "img_transforms = tt.Compose([tt.Resize((224, 224)),\n",
    "                               tt.ToTensor(),\n",
    "                               tt.Normalize(mean=d_means, std=d_stds)\n",
    "                               ])\n",
    "\n",
    "def get_checkpoints(question):\n",
    "    question.lower()\n",
    "    question = \"\".join([char for char in question.lower() if char not in punctuation])\n",
    "    question_split = question.split()\n",
    "    if question_split[0] == \"how\" and question_split[1] == \"many\":\n",
    "        return init_count_model_structure, init_count_vocab_to_int, init_count_ans_vocab_to_int\n",
    "    else:\n",
    "        return init_general_model_structure, init_general_vocab_to_int, init_general_ans_vocab_to_int\n",
    "\n",
    "def predict_image(question, img, model, ans_vocab_to_int):\n",
    "\n",
    "    # Convert to a batch of 1\n",
    "    input_img = to_device(img.unsqueeze(0), device)\n",
    "    input_question = to_device(torch.Tensor(question).to(torch.int64).unsqueeze(0), device)\n",
    "\n",
    "    # Get predictions from model\n",
    "    yb = model(input_img, input_question)\n",
    "\n",
    "    # Pick index with highest probability\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    \n",
    "    # Retrieve the class label\n",
    "    for answer, answer_int in ans_vocab_to_int.items():\n",
    "      if answer_int == preds[0].item():\n",
    "          return answer\n",
    "\n",
    "def pad_features_backend(encoded_question, seq):\n",
    "    # Return features of questions_ints, where each question is padded with 0's \n",
    "    # or truncated to the input seq_length.\n",
    "    padded_encoded_question = np.zeros((1, seq), dtype=int)\n",
    "    padded_encoded_question[0, -len(encoded_question):] = np.array(encoded_question)[:seq]\n",
    "    return np.reshape(padded_encoded_question, (seq, ))\n",
    "\n",
    "def show_example_backend(question, image, vocab_to_int):\n",
    "    plt.imshow(image.permute(1, 2, 0))\n",
    "    print(\"Encoded question:\", question)\n",
    "    decoded_q = \"\"\n",
    "    for word in question:\n",
    "      for w, num in vocab_to_int.items():\n",
    "        if word == num:\n",
    "          decoded_q += w + \" \"\n",
    "    print(\"Decoded question:\", decoded_q)\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    return \"<h1>Reqog</h1>\"\n",
    "\n",
    "@app.route(\"/get-prediction\", methods=[\"POST\"])\n",
    "def get_data():\n",
    "    img_file = request.files['image']\n",
    "    question = request.form['question']\n",
    "\n",
    "    # Read the image via file.stream\n",
    "    img = Image.open(img_file.stream)\n",
    "    img = img.convert('RGB')\n",
    "    img = img_transforms(img)\n",
    "    \n",
    "    model, vocab_to_int, ans_vocab_to_int = get_checkpoints(question)\n",
    "\n",
    "    question = \"\".join([char for char in question.lower() if char not in punctuation])\n",
    "    \n",
    "    question_split = question.split()\n",
    "    new_question = []\n",
    "    for word in question_split:\n",
    "        if word in vocab_to_int.keys():\n",
    "          new_question.append(word)\n",
    "        else:\n",
    "          new_question.append('<unk>')\n",
    "\n",
    "    question_enc = [vocab_to_int[word] for word in new_question]\n",
    "\n",
    "    # Change the sequence length according to the model\n",
    "    question_enc = pad_features_backend(question_enc, seq=25)\n",
    "    answer = predict_image(question_enc, img, model, ans_vocab_to_int)\n",
    "    show_example_backend(question_enc, img, vocab_to_int)\n",
    "    print('Predicted Answer:', answer)\n",
    "\n",
    "    return app.response_class(status=200, response=json.dumps(answer), mimetype='application/json')\n",
    "\n",
    "app.run()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "VQA_Reqog.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:reqog] *",
   "language": "python",
   "name": "conda-env-reqog-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
